from fsa import FSA
from fst import FST
import numpy as np
import json
import datetime


def build_editfst(alphabet, counts):
    """Build a weighted FST instance that implements one-edit-distance operations.

    You should use the add_transition() method of the FST class
    to build an FST that returns all misspellings of a word
    with one-edit-distance away.

    The transition weight should be set based on the number of times
    particular edit operations were observed in the data (e.g.,
    spelling-data.txt). You are recommended to estimate (smoothed)
    probabilities for each edit operation, and use its logarithm.
    You are free to experiment with weighting schemes, but remember
    that the transduce() method of FST assumes additive weights.

    Arguments:
    ----
    alphabet    All letters that we should recognize
    counts      Alignment counts generated by compute-weights.py
    """

    fst = FST()
    for letter in alphabet:
        p1 = np.log10((counts[letter][letter] / sum(list(counts[letter].values()))) + 1)

        # identity transitions of all letters
        fst.add_transition(s1=0, insym=letter, s2=0, outsym=letter,
                           w=p1, accepting=False)
        fst.add_transition(s1=1, insym=letter, s2=1, outsym=letter,
                           w=p1, accepting=True)
        # insertion
        fst.add_transition(s1=0, insym="", s2=1, outsym=letter,
                           w=np.log10((counts[""][letter] / sum(list(counts[""].values()))) + 1), accepting=True)
        # deletion
        fst.add_transition(s1=0, insym=letter, s2=1, outsym="",
                           w=np.log10((counts[letter][""] / sum(list(counts[letter].values()))) + 1), accepting=True)

        for letter2 in alphabet:
            if letter != letter2:
                # replacement of any letter with any other
                p2 = np.log10((counts[letter][letter2] / sum(list(counts[letter].values()))) + 1)
                fst.add_transition(s1=0, insym=letter, s2=1, outsym=letter2, w=p2, accepting=True)

    return fst


if __name__ == "__main__":
    # Example usage:

    # train lexicon
    with open('train_data/lexicon.txt', 'rt', encoding="utf8") as f:
        words = f.read().strip().split()
        alphabet = set("".join(words))

    # common spelling errors, from min. edit-distance alignment
    with open('train_data/spell-errors.json', 'rt') as f:
        errcount = json.loads(f.read())

    # trie lexicon
    print('built fsa lexicon!')
    fsa = FSA(deterministic=True)
    fsa.build_trie(words[: 1000])
    print('lexicon ready...\n')

    # minimize
    # estimate time
    print('minimize lexicon!\n'
          f'expected time: mm: 4.0, secs: 15\n%%%')

    start_time = datetime.datetime.now()
    fsa.minimize()
    end_time = datetime.datetime.now()
    hh, mm, sec = str(end_time - start_time).split(':')

    print('lexicon minimized...\n'
          f'time taken: hh: {round(float(hh), 2)}, mm: {round(float(mm), 2)}, secs: {round(float(sec), 3)}\n')

    # convert lexicon to FST
    print('build transducer of the lexicon!')
    lexicon = FST.fromfsa(fsa)
    print('transducer M1 ready...')

    # # Build the edit-distance FST

    # edits = build_editfst(letters, errcount)
    # print(3)

    # Compose them
    # spellfst = FST.compose_fst(lexicon, edits)
    # print(4)
    #
    # # The above generates all spelling mistakes, we want the invert
    # spellfst.invert()
    # spellfst.transduce("wort")
    # for sperr, w in sorted(spellfst.transduce("wort"),
    #                        key=lambda x: x[1], reverse=True):
    #     print(sperr, w)
