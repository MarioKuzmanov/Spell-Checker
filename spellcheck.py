import numpy as np
from fsa import FSA
from fst import FST
import json
import datetime


def build_editfst(alphabet, counts):
    """Build a weighted FST instance that implements one-edit-distance operations.

    You should use the add_transition() method of the FST class
    to build an FST that returns all misspellings of a word
    with one-edit-distance away.

    The transition weight should be set based on the number of times
    particular edit operations were observed in the data (e.g.,
    spelling-data.txt). You are recommended to estimate (smoothed)
    probabilities for each edit operation, and use its logarithm.
    You are free to experiment with weighting schemes, but remember
    that the transduce() method of FST assumes additive weights.

    Arguments:
    ----
    alphabet    All letters that we should recognize
    counts      Alignment counts generated by compute-weights.py
    """
    editfst = FST()
    for i in range(len(alphabet)):
        # identity mappings
        occ_identity, occ_total_identity = counts[alphabet[i]][alphabet[i]] + 0.05, sum(
            counts[alphabet[i]].values()) + (len(counts[alphabet[i]]) * 0.05)
        p_identity = occ_identity / occ_total_identity

        editfst.add_transition(s1=0, insym=alphabet[i], s2=0, outsym=alphabet[i], w=np.log10(p_identity),
                               accepting=False)
        editfst.add_transition(s1=1, insym=alphabet[i], s2=1, outsym=alphabet[i], w=np.log10(p_identity),
                               accepting=True)

        # deletion mappings
        occ_deletion, occ_total_deletion = counts[alphabet[i]][""] + 0.05, sum(counts[alphabet[i]].values()) + (
                len(counts[alphabet[i]]) * 0.05)
        p_deletion = occ_deletion / occ_total_deletion

        editfst.add_transition(s1=0, insym=alphabet[i], s2=1, outsym="", w=np.log10(p_deletion), accepting=True)

        # insertion mappings
        occ_insertion, occ_total_insertion = (counts[""][alphabet[i]] + 0.05, sum(counts[""].values()) +
                                              (len(counts[""]) * 0.05))
        p_insertion = occ_insertion / occ_total_insertion

        editfst.add_transition(s1=0, insym="", s2=1, outsym=alphabet[i], w=np.log10(p_insertion), accepting=True)
        for j in range(len(alphabet)):
            if i != j:
                # substitution mappings
                occ_substitution, occ_total_substitution = counts[alphabet[i]][alphabet[j]] + 0.05, sum(
                    counts[alphabet[i]].values()) + (len(counts[alphabet[i]]) * 0.05)
                p_substitution = occ_substitution / occ_total_substitution

                editfst.add_transition(s1=0, insym=alphabet[i], s2=1, outsym=alphabet[j], w=np.log10(p_substitution),
                                       accepting=True)
    return editfst


if __name__ == "__main__":
    # Example usage:

    # train lexicon
    with open('train_data/lexicon.txt', 'rt', encoding="utf8") as f:
        words = f.read().strip().split()
        alphabet = sorted(set("".join(words))) + [""]

    # common spelling errors, from min. edit-distance alignment
    with open('train_data/spell-errors.json', 'rt') as f:
        errcount = json.loads(f.read())

    # trie lexicon
    print('built fsa lexicon!')
    fsa = FSA(deterministic=True)
    fsa.build_trie(words[: 50])
    print('lexicon ready...\n')

    # minimize
    # estimate time
    print('minimize lexicon!\n'
          f'expected time: mm: 4.0, secs: 15\n%%%')

    start_time = datetime.datetime.now()
    fsa.minimize()
    end_time = datetime.datetime.now()
    hh, mm, sec = str(end_time - start_time).split(':')

    print('lexicon minimized...\n'
          f'time taken: hh: {round(float(hh), 2)}, mm: {round(float(mm), 2)}, secs: {round(float(sec), 3)}\n')

    # convert lexicon to FST
    print('build transducer on the lexicon!')
    lexicon = FST.fromfsa(fsa)
    print('M1 ready...')

    # build the edit-distance FST
    print('build misspellings transducer!')
    edits = build_editfst(alphabet, errcount)
    print('M2 is ready...\n%%%')

    # compose FSTs
    spellfst = FST.compose_fst(lexicon, edits)

    # The above generates all spelling mistakes, we want the invert
    # spellfst.invert()
    # spellfst.transduce("wort")
    # for sperr, w in sorted(spellfst.transduce("wort"),
    #                        key=lambda x: x[1], reverse=True):
    #     print(sperr, w)
