import numpy as np
from fsa import FSA
from fst import FST
import json
import datetime


class Spell_Checker:
    """
       Implements the pipeline to build a spell-checker
    """

    def __init__(self, lexicon='data/lexicon.txt', spell_errors='data/spell-errors.json'):
        self.fst = None
        self.l, self.se = lexicon, spell_errors
        self.build_pipeline()

    @staticmethod
    def build_editfst(alphabet, counts):

        """
        Weighted FST instance that implements one-edit-distance operations.

        The transition weight is based on the logarithm of the smoothed number of times
        particular edit operations were observed in the data.

        Arguments:
        ----
        alphabet    All letters that we should recognize
        counts      Alignment counts generated by compute_weights.py
        """
        editfst = FST()
        for i in range(len(alphabet)):
            # identity mappings
            occ_identity, occ_total_identity = counts[alphabet[i]][alphabet[i]] + 0.05, sum(
                counts[alphabet[i]].values()) + (len(counts[alphabet[i]]) * 0.05)
            p_identity = occ_identity / occ_total_identity

            editfst.add_transition(s1=0, insym=alphabet[i], s2=0, outsym=alphabet[i], w=np.log10(p_identity),
                                   accepting=False)
            editfst.add_transition(s1=1, insym=alphabet[i], s2=1, outsym=alphabet[i], w=np.log10(p_identity),
                                   accepting=True)

            # deletion mappings
            occ_deletion, occ_total_deletion = counts[alphabet[i]][""] + 0.05, sum(counts[alphabet[i]].values()) + (
                    len(counts[alphabet[i]]) * 0.05)
            p_deletion = occ_deletion / occ_total_deletion

            editfst.add_transition(s1=0, insym=alphabet[i], s2=1, outsym="", w=np.log10(p_deletion), accepting=True)

            # insertion mappings
            occ_insertion, occ_total_insertion = (counts[""][alphabet[i]] + 0.05, sum(counts[""].values()) +
                                                  (len(counts[""]) * 0.05))
            p_insertion = occ_insertion / occ_total_insertion

            editfst.add_transition(s1=0, insym="", s2=1, outsym=alphabet[i], w=np.log10(p_insertion), accepting=True)
            for j in range(len(alphabet)):
                if i != j:
                    # substitution mappings
                    occ_substitution, occ_total_substitution = counts[alphabet[i]][alphabet[j]] + 0.05, sum(
                        counts[alphabet[i]].values()) + (len(counts[alphabet[i]]) * 0.05)
                    p_substitution = occ_substitution / occ_total_substitution

                    editfst.add_transition(s1=0, insym=alphabet[i], s2=1, outsym=alphabet[j],
                                           w=np.log10(p_substitution),
                                           accepting=True)
        return editfst

    def build_pipeline(self):
        # train lexicon
        with open(self.l, 'rt', encoding="utf8") as f:
            words = f.read().strip().split()
            alphabet = sorted(set("".join(words))) + [""]

        # common spelling errors, from min. edit-distance alignment
        with open(self.se, 'rt', encoding='utf8') as f:
            errcount = json.loads(f.read())

        # trie lexicon
        print('built fsa lexicon!')
        fsa = FSA(deterministic=True)
        fsa.build_trie(words)
        print('lexicon ready...\n')

        # minimize
        # estimate time
        print('minimize lexicon!\n'
              f'expected time: mm: 4.0, secs: 15\n%%%')

        start_time = datetime.datetime.now()
        fsa.minimize()
        end_time = datetime.datetime.now()
        hh, mm, sec = str(end_time - start_time).split(':')

        print('lexicon minimized...\n'
              f'time taken: hh: {round(float(hh), 2)}, mm: {round(float(mm), 2)}, secs: {round(float(sec), 3)}\n')

        # convert lexicon to FST
        print('build transducer on the lexicon!')
        lexicon = FST.fromfsa(fsa)
        print('M1 ready...')

        # build the edit-distance FST
        print('build misspellings transducer!')
        edits = Spell_Checker.build_editfst(alphabet, errcount)
        print('M2 is ready...\n%%%')

        # compose FSTs, generates all spelling mistakes
        print('\ncompose lexicon with editfst!')
        spellfst = FST.compose_fst(lexicon, edits)
        print('invert!')
        # generates all corrections
        spellfst.invert()
        print('spell-fst is ready to use...\n')

        print('use `.fst.transduce` to see weighted spelling corrections...')
        self.fst = spellfst
